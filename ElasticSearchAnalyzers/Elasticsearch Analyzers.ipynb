{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font size=\"3\">\n",
    "    \n",
    "# Elasticsearch Analyzers\n",
    "\n",
    "## General\n",
    "For an Elasticsearch index, if no mapping is defined, each field is mapped as \"text\".\n",
    "\n",
    "Each field mapped as \"text\" is \"analyzed\" i.e. is processed by the Elasticsearch analyser.\n",
    "\n",
    "## Default analyzer\n",
    "If no custom analyzer is set for an indice, Elasticsearch applies a \"default analyzer\".\n",
    "\n",
    "## Elasticsearch analyzer chain\n",
    "Each Elasticsearch analyzer is a processing chain which can include following components:\n",
    "\n",
    "1. **Char Filters** ⇒ substitution of character sequences<br>\n",
    "ref: https://www.elastic.co/guide/en/elasticsearch/reference/current/analysis-mapping-charfilter.html<br>\n",
    "Not set in the standard analyser<br><br>\n",
    "\n",
    "2. **Tokenizer**<br>\n",
    "By default ES uses the Unicode Text segmentation algorithm<br>\n",
    "http://unicode.org/reports/tr29/<br><br>\n",
    "\n",
    "3. **Token filters**<br>\n",
    "ref: https://www.elastic.co/guide/en/elasticsearch/reference/current/analysis-tokenfilters.html<br><br>\n",
    "\n",
    "## Multiple analyzers\n",
    "When analyzers are defined on the index settings level, to apply a given analyzer for a field, this should be defined in the \"mapping\" field of the index settings.\n",
    "\n",
    "ref: https://www.elastic.co/guide/en/elasticsearch/reference/current/analyzer.html\n",
    "\n",
    "<br><br>\n",
    "</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sudo: pas de tty présent et pas de programme askpass spécifié\n",
      "\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "# Start a local Elasticsearch server\n",
    "$HOME/SCRIPTS/start_es.sh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
      "                                 Dload  Upload   Total   Spent    Left  Speed\n",
      "\n",
      "  0     0    0     0    0     0      0      0 --:--:-- --:--:-- --:--:--     0\n",
      "100   493  100   493    0     0   4522      0 --:--:-- --:--:-- --:--:--  4522\n",
      "HTTP/1.1 200 OK\n",
      "content-type: application/json; charset=UTF-8\n",
      "content-length: 493\n",
      "\n",
      "{\n",
      "  \"name\" : \"-zuCF-z\",\n",
      "  \"cluster_name\" : \"elasticsearch\",\n",
      "  \"cluster_uuid\" : \"rZX5yELHQfyX2bMjZr6wyw\",\n",
      "  \"version\" : {\n",
      "    \"number\" : \"6.6.1\",\n",
      "    \"build_flavor\" : \"default\",\n",
      "    \"build_type\" : \"tar\",\n",
      "    \"build_hash\" : \"1fd8f69\",\n",
      "    \"build_date\" : \"2019-02-13T17:10:04.160291Z\",\n",
      "    \"build_snapshot\" : false,\n",
      "    \"lucene_version\" : \"7.6.0\",\n",
      "    \"minimum_wire_compatibility_version\" : \"5.6.0\",\n",
      "    \"minimum_index_compatibility_version\" : \"5.0.0\"\n",
      "  },\n",
      "  \"tagline\" : \"You Know, for Search\"\n",
      "}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "# check if the server is running\n",
    "curl -X GET -i http://localhost:9200"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Creating custom analyzers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
      "                                 Dload  Upload   Total   Spent    Left  Speed\n",
      "\n",
      "  0     0    0     0    0     0      0      0 --:--:-- --:--:-- --:--:--     0\n",
      "100    21  100    21    0     0    141      0 --:--:-- --:--:-- --:--:--   142\n",
      "HTTP/1.1 200 OK\n",
      "content-type: application/json; charset=UTF-8\n",
      "content-length: 21\n",
      "\n",
      "{\"acknowledged\":true}  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
      "                                 Dload  Upload   Total   Spent    Left  Speed\n",
      "\n",
      "  0     0    0     0    0     0      0      0 --:--:-- --:--:-- --:--:--     0\n",
      "100  1123  100    67  100  1056    191   3025 --:--:-- --:--:-- --:--:--  3217\n",
      "HTTP/1.1 100 Continue\n",
      "\n",
      "HTTP/1.1 200 OK\n",
      "Warning: 299 Elasticsearch-6.6.1-1fd8f69 \"the default number of shards will change from [5] to [1] in 7.0.0; if you wish to continue using the default of [5] shards, you must manage this on the create index request or with an index template\" \"Tue, 01 Oct 2019 12:39:25 GMT\"\n",
      "content-type: application/json; charset=UTF-8\n",
      "content-length: 67\n",
      "\n",
      "{\"acknowledged\":true,\"shards_acknowledged\":true,\"index\":\"my_index\"}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "# delete settings if already set\n",
    "curl -X DELETE -i http://localhost:9200/my_index\n",
    "\n",
    "# create 3 analyzers\n",
    "curl -X PUT -H 'Content-Type: application/json' -i http://localhost:9200/my_index --data '{\n",
    "  \"settings\": {\n",
    "    \"analysis\": {\n",
    "      \"analyzer\": {\n",
    "        \"my_analyzer1\": {\n",
    "          \"tokenizer\": \"keyword\",\n",
    "          \"char_filter\": [\n",
    "            \"my_char_filter1\"\n",
    "          ]\n",
    "        },\n",
    "        \"my_analyzer2\": {\n",
    "          \"tokenizer\": \"standard\",\n",
    "          \"char_filter\": [\n",
    "            \"my_char_filter1\"\n",
    "          ],\n",
    "          \"filter\": [\"lowercase\"]\n",
    "        },\n",
    "        \"my_analyzer3\": {\n",
    "          \"tokenizer\": \"standard\",\n",
    "          \"char_filter\": [\n",
    "            \"my_char_filter2\"\n",
    "          ]\n",
    "        }\n",
    "      },\n",
    "      \"char_filter\": {\n",
    "        \"my_char_filter1\": {\n",
    "          \"type\": \"mapping\",\n",
    "          \"mappings\": [\n",
    "            \"٠ => 0\",\n",
    "            \"١ => 1\",\n",
    "            \"٢ => 2\",\n",
    "            \"٣ => 3\",\n",
    "            \"٤ => 4\",\n",
    "            \"٥ => 5\",\n",
    "            \"٦ => 6\",\n",
    "            \"٧ => 7\",\n",
    "            \"٨ => 8\",\n",
    "            \"٩ => 9\"\n",
    "          ]\n",
    "        },\n",
    "        \"my_char_filter2\": {\n",
    "          \"type\": \"mapping\",\n",
    "          \"mappings\": [\n",
    "            \":) => _happy_\",\n",
    "            \":( => _sad_\"\n",
    "          ]\n",
    "        }\n",
    "      }\n",
    "    }\n",
    "  }\n",
    "}'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# What does the standard analyzer do"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
      "                                 Dload  Upload   Total   Spent    Left  Speed\n",
      "\n",
      "  0     0    0     0    0     0      0      0 --:--:-- --:--:-- --:--:--     0\n",
      "100   768  100   687  100    81  10253   1208 --:--:-- --:--:-- --:--:-- 11462\n",
      "\"it\"\n",
      "\"even\"\n",
      "\"appeared\"\n",
      "\"recently\"\n",
      "\"in\"\n",
      "\"graphic\"\n",
      "\"novel\"\n",
      "\"form\"\n",
      "\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "curl -X POST -H 'Content-Type: application/json' \\\n",
    "    http://localhost:9200/my_index/_analyze \\\n",
    "    -d '{\"analyzer\": \"standard\",\"text\":\"It even appeared recently in graphic-novel form\"}'|jq '.tokens[].token'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Testing my_analyzer1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
      "                                 Dload  Upload   Total   Spent    Left  Speed\n",
      "\n",
      "  0     0    0     0    0     0      0      0 --:--:-- --:--:-- --:--:--     0\n",
      "100   181  100   110  100    71   5238   3380 --:--:-- --:--:-- --:--:--  8619\n",
      "\"My license plate is 25015\"\n",
      "\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "\n",
    "curl -X POST -H 'Content-Type: application/json' \\\n",
    "    http://localhost:9200/my_index/_analyze \\\n",
    "    -d '{\"analyzer\": \"my_analyzer1\", \"text\": \"My license plate is ٢٥٠١٥\" }'|jq '.tokens[].token'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Testing my_analyzer2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
      "                                 Dload  Upload   Total   Spent    Left  Speed\n",
      "\n",
      "  0     0    0     0    0     0      0      0 --:--:-- --:--:-- --:--:--     0\n",
      "100   496  100   425  100    71  28333   4733 --:--:-- --:--:-- --:--:-- 33066\n",
      "\"my\"\n",
      "\"license\"\n",
      "\"plate\"\n",
      "\"is\"\n",
      "\"25015\"\n",
      "\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "\n",
    "curl -X POST -H 'Content-Type: application/json' \\\n",
    "    http://localhost:9200/my_index/_analyze \\\n",
    "    -d '{\"analyzer\": \"my_analyzer2\", \"text\": \"My license plate is ٢٥٠١٥\" }'|jq '.tokens[].token'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Testing my_analyzer3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
      "                                 Dload  Upload   Total   Spent    Left  Speed\n",
      "\n",
      "  0     0    0     0    0     0      0      0 --:--:-- --:--:-- --:--:--     0\n",
      "100   497  100   432  100    65  22736   3421 --:--:-- --:--:-- --:--:-- 26157\n",
      "\"Im\"\n",
      "\"delighted\"\n",
      "\"about\"\n",
      "\"it\"\n",
      "\"_sad_\"\n",
      "\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "\n",
    "curl -X POST -H 'Content-Type: application/json' \\\n",
    "    http://localhost:9200/my_index/_analyze \\\n",
    "    -d '{\"analyzer\": \"my_analyzer3\", \"text\": \"I''m delighted about it :(\" }'|jq '.tokens[].token'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font size=\"3\">\n",
    "    \n",
    "# Elasticsearch ngrams and shingles\n",
    "\n",
    "## Elasticsearch ngrams\n",
    "\n",
    "On Elasticsearch level, \"ngram\" is a kind of tokenizer is a way to full text indexing with tokens built as a sequence of characters more than sequence of words.\n",
    "\n",
    "To know more about Elasticsearch \"ngram\" tokenization, read:<br>\n",
    "https://www.elastic.co/guide/en/elasticsearch/reference/current/analysis-ngram-tokenizer.html\n",
    "\n",
    "## Elasticsearch shingles\n",
    "\n",
    "It as also possible to tokens based on sequences of words provided that our analyzer includes a \"shingle\" filter.\n",
    "\n",
    "</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
      "                                 Dload  Upload   Total   Spent    Left  Speed\n",
      "\n",
      "  0     0    0     0    0     0      0      0 --:--:-- --:--:-- --:--:--     0\n",
      "100    21  100    21    0     0    304      0 --:--:-- --:--:-- --:--:--   304\n",
      "HTTP/1.1 200 OK\n",
      "content-type: application/json; charset=UTF-8\n",
      "content-length: 21\n",
      "\n",
      "{\"acknowledged\":true}  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
      "                                 Dload  Upload   Total   Spent    Left  Speed\n",
      "\n",
      "  0     0    0     0    0     0      0      0 --:--:-- --:--:-- --:--:--     0\n",
      "100   822  100    67  100   755    435   4902 --:--:-- --:--:-- --:--:--  5337\n",
      "HTTP/1.1 200 OK\n",
      "Warning: 299 Elasticsearch-6.6.1-1fd8f69 \"Deprecated big difference between maxShingleSize and minShingleSize in Shingle TokenFilter,expected difference must be less than or equal to: [3]\" \"Tue, 01 Oct 2019 12:40:02 GMT\"\n",
      "content-type: application/json; charset=UTF-8\n",
      "content-length: 67\n",
      "\n",
      "{\"acknowledged\":true,\"shards_acknowledged\":true,\"index\":\"my_index\"}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "\n",
    "# delete settings if already set\n",
    "curl -X DELETE -i http://localhost:9200/my_index\n",
    "\n",
    "# Exemple of analyzer with shingle filter\n",
    "curl -X PUT -H 'Content-Type: application/json' -i http://localhost:9200/my_index --data '{\n",
    "  \"settings\": {\n",
    "    \"index\" : {\n",
    "      \"number_of_shards\" : \"1\",\n",
    "      \"number_of_replicas\" : \"0\"\n",
    "    },\n",
    "    \"analysis\": {\n",
    "      \"analyzer\": {\n",
    "        \"shingle5\": {\n",
    "          \"type\": \"custom\",\n",
    "          \"tokenizer\": \"standard\",\n",
    "          \"filter\": [\"lowercase\", \"my_stop\", \"shingle5_filter\"]\n",
    "        }\n",
    "      },\n",
    "      \"filter\": {\n",
    "        \"shingle5_filter\": {\n",
    "          \"type\": \"shingle\",\n",
    "          \"min_shingle_size\": 2,\n",
    "          \"max_shingle_size\": 5\n",
    "        },\n",
    "        \"my_stop\": {\n",
    "            \"type\":       \"stop\",\n",
    "            \"stopwords\":  \"_english_\"\n",
    "        }\n",
    "      }\n",
    "    }\n",
    "  },\n",
    "  \"mappings\":{\n",
    "    \"my_mapping\": {\n",
    "      \"properties\":{\n",
    "         \"doc_txt\": {\n",
    "            \"type\":\"text\",\n",
    "            \"analyzer\":\"shingle5\"\n",
    "         }\n",
    "      }\n",
    "    }\n",
    "  }\n",
    "}'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
      "                                 Dload  Upload   Total   Spent    Left  Speed\n",
      "\n",
      "  0     0    0     0    0     0      0      0 --:--:-- --:--:-- --:--:--     0\n",
      "100  3129  100  3048  100    81   156k   4263 --:--:-- --:--:-- --:--:--  160k\n",
      "\"_ even\"\n",
      "\"_ even appeared\"\n",
      "\"_ even appeared recently\"\n",
      "\"_ even appeared recently _\"\n",
      "\"even\"\n",
      "\"even appeared\"\n",
      "\"even appeared recently\"\n",
      "\"even appeared recently _\"\n",
      "\"even appeared recently _ graphic\"\n",
      "\"appeared\"\n",
      "\"appeared recently\"\n",
      "\"appeared recently _\"\n",
      "\"appeared recently _ graphic\"\n",
      "\"appeared recently _ graphic novel\"\n",
      "\"recently\"\n",
      "\"recently _\"\n",
      "\"recently _ graphic\"\n",
      "\"recently _ graphic novel\"\n",
      "\"recently _ graphic novel form\"\n",
      "\"_ graphic\"\n",
      "\"_ graphic novel\"\n",
      "\"_ graphic novel form\"\n",
      "\"graphic\"\n",
      "\"graphic novel\"\n",
      "\"graphic novel form\"\n",
      "\"novel\"\n",
      "\"novel form\"\n",
      "\"form\"\n",
      "\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "\n",
    "# Apply the shingle5 analyzer:\n",
    "curl -X POST -H 'Content-Type: application/json' \\\n",
    "    http://localhost:9200/my_index/_analyze -d '{\"analyzer\": \"shingle5\",\"text\":\"It even appeared recently in graphic-novel form\"}'|jq '.tokens[].token'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
      "                                 Dload  Upload   Total   Spent    Left  Speed\n",
      "\n",
      "  0     0    0     0    0     0      0      0 --:--:-- --:--:-- --:--:--     0\n",
      "100  4294  100  4209  100    85   456k   9444 --:--:-- --:--:-- --:--:--  465k\n",
      "\"_ graphic\"\n",
      "\"_ graphic novel\"\n",
      "\"_ graphic novel _\"\n",
      "\"_ graphic novel _ _\"\n",
      "\"graphic\"\n",
      "\"graphic novel\"\n",
      "\"graphic novel _\"\n",
      "\"graphic novel _ _\"\n",
      "\"graphic novel _ _ book\"\n",
      "\"novel\"\n",
      "\"novel _\"\n",
      "\"novel _ _\"\n",
      "\"novel _ _ book\"\n",
      "\"novel _ _ book made\"\n",
      "\"_ _ book\"\n",
      "\"_ _ book made\"\n",
      "\"_ _ book made up\"\n",
      "\"_ book\"\n",
      "\"_ book made\"\n",
      "\"_ book made up\"\n",
      "\"_ book made up _\"\n",
      "\"book\"\n",
      "\"book made\"\n",
      "\"book made up\"\n",
      "\"book made up _\"\n",
      "\"book made up _ comics\"\n",
      "\"made\"\n",
      "\"made up\"\n",
      "\"made up _\"\n",
      "\"made up _ comics\"\n",
      "\"made up _ comics content\"\n",
      "\"up\"\n",
      "\"up _\"\n",
      "\"up _ comics\"\n",
      "\"up _ comics content\"\n",
      "\"_ comics\"\n",
      "\"_ comics content\"\n",
      "\"comics\"\n",
      "\"comics content\"\n",
      "\"content\"\n",
      "\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "\n",
    "# Apply the shingle5 analyzer:\n",
    "curl -X POST -H 'Content-Type: application/json' \\\n",
    "    http://localhost:9200/my_index/_analyze -d '{\"analyzer\": \"shingle5\",\"text\":\"A graphic novel is a book made up of comics content\"}'|jq '.tokens[].token'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
      "                                 Dload  Upload   Total   Spent    Left  Speed\n",
      "\n",
      "  0     0    0     0    0     0      0      0 --:--:-- --:--:-- --:--:--     0\n",
      "100   228  100   162  100    66   1396    568 --:--:-- --:--:-- --:--:--  1965\n",
      "{\"_index\":\"my_index\",\"_type\":\"my_mapping\",\"_id\":\"1\",\"_version\":1,\"result\":\"created\",\"_shards\":{\"total\":1,\"successful\":1,\"failed\":0},\"_seq_no\":0,\"_primary_term\":1}  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
      "                                 Dload  Upload   Total   Spent    Left  Speed\n",
      "\n",
      "  0     0    0     0    0     0      0      0 --:--:-- --:--:-- --:--:--     0\n",
      "100   232  100   162  100    70   7363   3181 --:--:-- --:--:-- --:--:-- 10545\n",
      "{\"_index\":\"my_index\",\"_type\":\"my_mapping\",\"_id\":\"2\",\"_version\":1,\"result\":\"created\",\"_shards\":{\"total\":1,\"successful\":1,\"failed\":0},\"_seq_no\":1,\"_primary_term\":1}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "\n",
    "# insert first document\n",
    "\n",
    "curl -X PUT -H 'Content-Type: application/json' http://localhost:9200/my_index/my_mapping/1 --data '{\n",
    "  \"doc_txt\": \"It even appeared recently in graphic-novel form\"\n",
    "}'\n",
    "\n",
    "#  insert second document\n",
    "\n",
    "curl -X PUT -H 'Content-Type: application/json' http://localhost:9200/my_index/my_mapping/2 --data '{\n",
    "  \"doc_txt\": \"A graphic novel is a book made up of comics content\"\n",
    "}'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
      "                                 Dload  Upload   Total   Spent    Left  Speed\n",
      "\n",
      "  0     0    0     0    0     0      0      0 --:--:-- --:--:-- --:--:--     0\n",
      "100   471  100   423  100    48   5794    657 --:--:-- --:--:-- --:--:--  6452\n",
      "{\n",
      "  \"took\": 57,\n",
      "  \"timed_out\": false,\n",
      "  \"_shards\": {\n",
      "    \"total\": 1,\n",
      "    \"successful\": 1,\n",
      "    \"skipped\": 0,\n",
      "    \"failed\": 0\n",
      "  },\n",
      "  \"hits\": {\n",
      "    \"total\": 2,\n",
      "    \"max_score\": 1,\n",
      "    \"hits\": [\n",
      "      {\n",
      "        \"_index\": \"my_index\",\n",
      "        \"_type\": \"my_mapping\",\n",
      "        \"_id\": \"1\",\n",
      "        \"_score\": 1,\n",
      "        \"_source\": {\n",
      "          \"doc_txt\": \"It even appeared recently in graphic-novel form\"\n",
      "        }\n",
      "      },\n",
      "      {\n",
      "        \"_index\": \"my_index\",\n",
      "        \"_type\": \"my_mapping\",\n",
      "        \"_id\": \"2\",\n",
      "        \"_score\": 1,\n",
      "        \"_source\": {\n",
      "          \"doc_txt\": \"A graphic novel is a book made up of comics content\"\n",
      "        }\n",
      "      }\n",
      "    ]\n",
      "  }\n",
      "}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "\n",
    "# check if data has been inserted\n",
    "curl -X POST -H 'Content-Type: application/json' http://localhost:9200/my_index/my_mapping/_search --data '{\n",
    "    \"query\": {\n",
    "        \"match_all\": {}\n",
    "    }\n",
    "}'|jq '.'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "method": "display_data"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0a2d4d16-200f-4e74-a170-5736f3397639",
       "version_major": 2,
       "version_minor": 0
      },
      "method": "display_data"
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%%classpath add mvn\n",
    "org.apache.lucene lucene-core 7.6.0\n",
    "org.elasticsearch.client elasticsearch-rest-high-level-client 6.6.1\n",
    "org.elasticsearch elasticsearch 6.6.1\n",
    "org.apache.logging.log4j log4j-core 2.11.2\n",
    "com.google.code.gson gson 2.8.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "com.twosigma.beaker.javash.bkr451fa51c.ElasticClient"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import org.elasticsearch.client.RestHighLevelClient;\n",
    "import org.elasticsearch.client.RestClient;\n",
    "import org.apache.http.HttpHost;\n",
    "import org.elasticsearch.action.search.SearchRequest;\n",
    "import org.elasticsearch.search.builder.SearchSourceBuilder;\n",
    "import org.elasticsearch.index.query.QueryBuilders;\n",
    "import org.elasticsearch.search.builder.SearchSourceBuilder;\n",
    "import org.elasticsearch.action.search.SearchResponse;\n",
    "import org.elasticsearch.action.admin.indices.get.GetIndexRequest;\n",
    "import org.elasticsearch.action.admin.indices.get.GetIndexResponse;\n",
    "import org.elasticsearch.action.admin.indices.settings.get.GetSettingsRequest;\n",
    "import org.elasticsearch.action.admin.indices.settings.get.GetSettingsResponse;\n",
    "import org.elasticsearch.client.RequestOptions;\n",
    "import org.elasticsearch.client.Response;\n",
    "import com.google.gson.Gson;\n",
    "import com.google.gson.GsonBuilder;\n",
    "import com.google.gson.JsonParser;\n",
    "import com.google.gson.JsonElement;\n",
    "import com.google.gson.JsonObject;\n",
    "import java.io.IOException;\n",
    "import org.apache.commons.lang3.tuple.Pair;\n",
    "import org.apache.commons.lang3.tuple.MutablePair;\n",
    "import java.io.InputStream;\n",
    "\n",
    "public class ElasticClient {\n",
    "    private String esServer;\n",
    "    private Integer esPort;\n",
    "    private RestHighLevelClient client = null;\n",
    "    private static final Gson GSONPRETTY = new GsonBuilder().setPrettyPrinting().create();\n",
    "    private static final JsonParser JP = new JsonParser();\n",
    "    \n",
    "    ElasticClient(String esServer, Integer esPort) {\n",
    "        this.esServer = esServer;\n",
    "        this.esPort = esPort;\n",
    "    }\n",
    "    \n",
    "    public RestHighLevelClient getClient() {\n",
    "        if (client == null) {\n",
    "            client = new RestHighLevelClient(RestClient.builder(new HttpHost(esServer, esPort, \"http\")));\n",
    "        }\n",
    "        return client;\n",
    "    }\n",
    "    \n",
    "    public void searchAll(String index, String query) throws IOException {\n",
    "        SearchRequest searchRequest = new SearchRequest(index);\n",
    "        SearchSourceBuilder searchSourceBuilder = new SearchSourceBuilder();\n",
    "        searchSourceBuilder.query(QueryBuilders.matchAllQuery());\n",
    "        searchRequest.source(searchSourceBuilder);\n",
    "        SearchResponse response = getClient().search(searchRequest);\n",
    "        jsonPrettyPrint(response.toString());\n",
    "    }\n",
    "    \n",
    "    public static void jsonPrettyPrint(String json) {\n",
    "        JsonElement je = JP.parse(json);\n",
    "        System.out.println(GSONPRETTY.toJson(je));\n",
    "    }\n",
    "    \n",
    "    public String[] getAllIndices() throws IOException {\n",
    "        GetIndexRequest request = new GetIndexRequest().indices(\"*\");\n",
    "        GetIndexResponse response = client.indices().get(request, RequestOptions.DEFAULT);\n",
    "        return response.getIndices();\n",
    "    }\n",
    "    \n",
    "    public Pair<String, Integer> getIndexProperties(String index) throws IOException {\n",
    "        GetSettingsRequest request = new GetSettingsRequest().indices(index);\n",
    "        GetSettingsResponse response = client.indices().getSettings(request, RequestOptions.DEFAULT);\n",
    "        JsonElement je = JP.parse(response.toString());\n",
    "        JsonObject indexObj = je.getAsJsonObject().get(index).getAsJsonObject().get(\"settings\").getAsJsonObject().get(\"index\").getAsJsonObject();\n",
    "        return new MutablePair<>(indexObj.get(\"uuid\").getAsString(), indexObj.get(\"number_of_shards\").getAsInt());\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR StatusLogger No Log4j 2 configuration file found. Using default configuration (logging only errors to the console), or user programmatically provided configurations. Set system property 'log4j2.debug' to show Log4j 2 internal initialization logging. See https://logging.apache.org/log4j/2.x/manual/configuration.html for instructions on how to configure Log4j 2\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "  \"took\": 2,\n",
      "  \"timed_out\": false,\n",
      "  \"_shards\": {\n",
      "    \"total\": 1,\n",
      "    \"successful\": 1,\n",
      "    \"skipped\": 0,\n",
      "    \"failed\": 0\n",
      "  },\n",
      "  \"hits\": {\n",
      "    \"total\": 2,\n",
      "    \"max_score\": 1.0,\n",
      "    \"hits\": [\n",
      "      {\n",
      "        \"_index\": \"my_index\",\n",
      "        \"_type\": \"my_mapping\",\n",
      "        \"_id\": \"1\",\n",
      "        \"_score\": 1.0,\n",
      "        \"_source\": {\n",
      "          \"doc_txt\": \"It even appeared recently in graphic-novel form\"\n",
      "        }\n",
      "      },\n",
      "      {\n",
      "        \"_index\": \"my_index\",\n",
      "        \"_type\": \"my_mapping\",\n",
      "        \"_id\": \"2\",\n",
      "        \"_score\": 1.0,\n",
      "        \"_source\": {\n",
      "          \"doc_txt\": \"A graphic novel is a book made up of comics content\"\n",
      "        }\n",
      "      }\n",
      "    ]\n",
      "  }\n",
      "}\n",
      "\n",
      "[.kibana_1, my_index, nutch]\n",
      "\n",
      "(YtgaGpRpTCyDhEtcY6SonQ,1)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "null"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import java.util.Arrays;\n",
    "import org.apache.commons.lang3.tuple.Pair;\n",
    "\n",
    "ElasticClient esCli = new ElasticClient(\"localhost\", 9200);\n",
    "esCli.searchAll(\"my_index\", \"\");\n",
    "System.out.println();\n",
    "System.out.println(Arrays.toString(esCli.getAllIndices()));\n",
    "System.out.println();\n",
    "Pair<String, Integer> myIndexSettings = esCli.getIndexProperties(\"my_index\");\n",
    "NamespaceClient.getBeakerX().set(\"my_index_uuid\", myIndexSettings.getKey());\n",
    "NamespaceClient.getBeakerX().set(\"my_index_shards\", myIndexSettings.getValue());\n",
    "System.out.println(myIndexSettings);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "com.twosigma.beaker.javash.bkr451fa51c.StringUtil"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "/*******************************************************************************\n",
    " * Copyright (c) 2010, 2012 Institute for Dutch Lexicology\n",
    " *\n",
    " * Licensed under the Apache License, Version 2.0 (the \"License\");\n",
    " * you may not use this file except in compliance with the License.\n",
    " * You may obtain a copy of the License at\n",
    " *\n",
    " *     http://www.apache.org/licenses/LICENSE-2.0\n",
    " *\n",
    " * Unless required by applicable law or agreed to in writing, software\n",
    " * distributed under the License is distributed on an \"AS IS\" BASIS,\n",
    " * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
    " * See the License for the specific language governing permissions and\n",
    " * limitations under the License.\n",
    " *******************************************************************************/\n",
    "import java.text.Collator;\n",
    "import java.text.Normalizer;\n",
    "import java.util.ArrayList;\n",
    "import java.util.Iterator;\n",
    "import java.util.List;\n",
    "import java.util.Locale;\n",
    "import java.util.Map;\n",
    "import java.util.regex.Matcher;\n",
    "import java.util.regex.Pattern;\n",
    "import org.apache.lucene.index.IndexReader;\n",
    "\n",
    "import org.apache.commons.lang3.StringUtils;\n",
    "\n",
    "/**\n",
    " * A collection of String-related utility methods and regular expression patterns.\n",
    " */\n",
    "public class StringUtil {\n",
    "    \n",
    "    final static Pattern pattern = Pattern.compile(\"\\\\p{InCombiningDiacriticalMarks}+\");\n",
    "\n",
    "\n",
    "    private StringUtil() {}\n",
    "\n",
    "    /**\n",
    "     * <p>Removes diacritics (~= accents) from a string. The case will not be altered.</p>\n",
    "     * <p>For instance, '&agrave;' will be replaced by 'a'.</p>\n",
    "     * <p>Note that ligatures will be left as is.</p>\n",
    "     *\n",
    "     * <pre>\n",
    "     * StringUtils.stripAccents(null)                = null\n",
    "     * StringUtils.stripAccents(\"\")                  = \"\"\n",
    "     * StringUtils.stripAccents(\"control\")           = \"control\"\n",
    "     * StringUtils.stripAccents(\"&eacute;clair\")     = \"eclair\"\n",
    "     * </pre>\n",
    "     *\n",
    "     * NOTE: this method was copied from Apache StringUtils. The only change is precompiling\n",
    "     * the regular expression for efficiency.\n",
    "     *\n",
    "     * @param input String to be stripped\n",
    "     * @return input text with diacritics removed\n",
    "     *\n",
    "     * @since 3.0\n",
    "     */\n",
    "    // See also Lucene's ASCIIFoldingFilter (Lucene 2.9) that replaces accented characters by their unaccented equivalent (and uncommitted bug fix: https://issues.apache.org/jira/browse/LUCENE-1343?focusedCommentId=12858907&page=com.atlassian.jira.plugin.system.issuetabpanels%3Acomment-tabpanel#action_12858907).\n",
    "    public static String stripAccents(final String input) {\n",
    "        if(input == null) {\n",
    "            return null;\n",
    "        }\n",
    "        final StringBuilder decomposed = new StringBuilder(Normalizer.normalize(input, Normalizer.Form.NFD));\n",
    "        convertRemainingAccentCharacters(decomposed);\n",
    "        // Note that this doesn't correctly remove ligatures...\n",
    "        return pattern.matcher(decomposed).replaceAll(StringUtils.EMPTY);\n",
    "    }\n",
    "\n",
    "    private static void convertRemainingAccentCharacters(StringBuilder decomposed) {\n",
    "        for (int i = 0; i < decomposed.length(); i++) {\n",
    "            if (decomposed.charAt(i) == '\\u0141') {\n",
    "                decomposed.deleteCharAt(i);\n",
    "                decomposed.insert(i, 'L');\n",
    "            } else if (decomposed.charAt(i) == '\\u0142') {\n",
    "                decomposed.deleteCharAt(i);\n",
    "                decomposed.insert(i, 'l');\n",
    "            }\n",
    "        }\n",
    "    }\n",
    "\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "com.twosigma.beaker.javash.bkr451fa51c.LuceneExplorer"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import java.util.List;\n",
    "import java.util.Map;\n",
    "import java.util.TreeMap;\n",
    "import java.util.ArrayList;\n",
    "import org.apache.lucene.index.DirectoryReader;\n",
    "import org.apache.lucene.index.IndexReader;\n",
    "import org.apache.lucene.index.LeafReaderContext;\n",
    "import org.apache.lucene.index.Terms;\n",
    "import org.apache.lucene.index.TermsEnum;\n",
    "import org.apache.lucene.search.IndexSearcher;\n",
    "import org.apache.lucene.store.SimpleFSDirectory;\n",
    "import org.apache.lucene.util.BytesRef;\n",
    "import java.nio.file.Paths;\n",
    "import java.io.IOException;\n",
    "import org.apache.commons.text.StringEscapeUtils;\n",
    "import org.apache.commons.lang3.StringUtils;\n",
    "import org.apache.logging.log4j.Logger;\n",
    "import org.apache.logging.log4j.LogManager;\n",
    "import java.nio.charset.Charset;\n",
    "\n",
    "public class LuceneExplorer {\n",
    "    \n",
    "    public static String USR_HOME = (String)System.getProperty(\"user.home\");\n",
    "    public static String ES_INSTALL_PATH = String.format(\"%s/TOOLS/BigData/es-661\", USR_HOME);\n",
    "    private static final Logger logger = LogManager.getLogger(LuceneExplorer.class);\n",
    "    static final Charset LUCENE_DEFAULT_CHARSET = Charset.forName(\"utf-8\");\n",
    "    \n",
    "    private String indexUuid;\n",
    "    private Integer indexNbrOfShards;\n",
    "    \n",
    "    public static void setEsInstallPath(String path) {\n",
    "        ES_INSTALL_PATH = path;\n",
    "    }\n",
    "    \n",
    "    LuceneExplorer(String uuid, Integer nbrShards) {\n",
    "        indexUuid = uuid;\n",
    "        indexNbrOfShards = nbrShards;\n",
    "    }\n",
    "    \n",
    "    public List<String> getLucenePaths() {\n",
    "        List<String> result = new ArrayList<String>();\n",
    "        for (int i=0; i<indexNbrOfShards; i++) {\n",
    "            result.add(String.format(\"%s/data/nodes/0/indices/%s/%d/index\", ES_INSTALL_PATH, indexUuid, i));\n",
    "        }\n",
    "        return result;\n",
    "    }\n",
    "    \n",
    "    final static boolean isTermCandidate(String term, int freq) throws Exception {\n",
    "//         return !StringUtils.isNumericSpace(term) && \n",
    "//                 freq > 5;\n",
    "        return !StringUtils.isNumericSpace(term);\n",
    "    }\n",
    "\n",
    "    /**\n",
    "     * Find terms in the index based on a prefix. Useful for autocomplete.\n",
    "     * @param index the index\n",
    "     * @param fieldName the field\n",
    "     * @param prefix the prefix we're looking for (null or empty string for all terms)\n",
    "     * @param sensitive match case-sensitively or not?\n",
    "     * @param maxResults max. number of results to return (or -1 for all)\n",
    "     * @return the matching terms\n",
    "     */\n",
    "    public static Map<String, Integer> findTermsByPrefix(IndexReader index, String fieldName,\n",
    "        String prefix, boolean sensitive, int maxResults) {\n",
    "        boolean allTerms = prefix == null || prefix.length() == 0;\n",
    "        if (allTerms) {\n",
    "            prefix = \"\";\n",
    "            sensitive = true; // don't do unnecessary work in this case\n",
    "        }\n",
    "        try {\n",
    "            if (!sensitive)\n",
    "                prefix = StringUtil.stripAccents(prefix).toLowerCase();\n",
    "            Map<String, Integer> results = new TreeMap<String, Integer>();\n",
    "            for (LeafReaderContext leafReader: index.leaves()) {\n",
    "                Terms terms = leafReader.reader().terms(fieldName);\n",
    "                if (terms == null) {\n",
    "                    if (logger.isDebugEnabled()) logger.debug(\"no terms for field \" + fieldName + \" in leafReader, skipping\");\n",
    "                    continue;\n",
    "                }\n",
    "                TermsEnum termsEnum = terms.iterator();\n",
    "                BytesRef brPrefix = new BytesRef(prefix.getBytes(LUCENE_DEFAULT_CHARSET));\n",
    "                TermsEnum.SeekStatus seekStatus = termsEnum.seekCeil(brPrefix);\n",
    "\n",
    "                if (seekStatus == TermsEnum.SeekStatus.END) {\n",
    "                    continue;\n",
    "                }\n",
    "                for (BytesRef term = termsEnum.term(); term != null; term = termsEnum.next()) {\n",
    "                    if (maxResults < 0 || results.size() < maxResults) {\n",
    "                        try {\n",
    "                            int freq = termsEnum.docFreq();\n",
    "                            String termText = term.utf8ToString();\n",
    "                            if (isTermCandidate(termText, freq)) {\n",
    "                                boolean startsWithPrefix = sensitive ? StringUtil.stripAccents(termText).startsWith(prefix) : termText.startsWith(prefix);\n",
    "                                if (!allTerms && !startsWithPrefix) {\n",
    "                                    // Doesn't match prefix or different field; no more matches\n",
    "                                    break;\n",
    "                                }\n",
    "                                // Match, add term\n",
    "                                if (results.get(termText) == null)\n",
    "                                    results.put(termText, freq);\n",
    "                            }\n",
    "                        } catch (Exception e) {}\n",
    "                    }\n",
    "                }\n",
    "            }\n",
    "            return results;\n",
    "        } catch (IOException e) {\n",
    "            throw new RuntimeException(e);\n",
    "        }\n",
    "    }\n",
    "\n",
    "    public void exploreLuceneIndex(String indexPath) throws IOException {\n",
    "        SimpleFSDirectory dir = new SimpleFSDirectory(Paths.get(indexPath));\n",
    "        IndexSearcher searcher = new IndexSearcher(DirectoryReader.open(dir));\n",
    "        IndexReader reader = searcher.getIndexReader();\n",
    "        int shardCardinal = reader.getDocCount(\"doc_txt\");\n",
    "        System.out.println(String.format(\"lucene index: %s\\ntotal docs:%d\", indexPath, shardCardinal));\n",
    "        Map<String, Integer> terms = findTermsByPrefix(reader, \"doc_txt\", \"\", true, -1);\n",
    "        terms.entrySet().stream().forEach(e -> System.out.println(\"\\\"\" + StringEscapeUtils.escapeJava(e.getKey()) + \"\\\",\" + e.getValue()));\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ElasticSearch has just been stopped\n",
      "\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "# Stop Elasticsearch server\n",
    "$HOME/SCRIPTS/stop_es.sh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "YtgaGpRpTCyDhEtcY6SonQ\n",
      "1\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR StatusLogger No Log4j 2 configuration file found. Using default configuration (logging only errors to the console), or user programmatically provided configurations. Set system property 'log4j2.debug' to show Log4j 2 internal initialization logging. See https://logging.apache.org/log4j/2.x/manual/configuration.html for instructions on how to configure Log4j 2\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[/home/weborama.office/cverdier/TOOLS/BigData/es-661/data/nodes/0/indices/YtgaGpRpTCyDhEtcY6SonQ/0/index]\n",
      "\n",
      "lucene index: /home/weborama.office/cverdier/TOOLS/BigData/es-661/data/nodes/0/indices/YtgaGpRpTCyDhEtcY6SonQ/0/index\n",
      "total docs:2\n",
      "\"_ _ book\",1\n",
      "\"_ _ book made\",1\n",
      "\"_ _ book made up\",1\n",
      "\"_ book\",1\n",
      "\"_ book made\",1\n",
      "\"_ book made up\",1\n",
      "\"_ book made up _\",1\n",
      "\"_ comics\",1\n",
      "\"_ comics content\",1\n",
      "\"_ even\",1\n",
      "\"_ even appeared\",1\n",
      "\"_ even appeared recently\",1\n",
      "\"_ even appeared recently _\",1\n",
      "\"_ graphic\",2\n",
      "\"_ graphic novel\",2\n",
      "\"_ graphic novel _\",1\n",
      "\"_ graphic novel _ _\",1\n",
      "\"_ graphic novel form\",1\n",
      "\"appeared\",1\n",
      "\"appeared recently\",1\n",
      "\"appeared recently _\",1\n",
      "\"appeared recently _ graphic\",1\n",
      "\"appeared recently _ graphic novel\",1\n",
      "\"book\",1\n",
      "\"book made\",1\n",
      "\"book made up\",1\n",
      "\"book made up _\",1\n",
      "\"book made up _ comics\",1\n",
      "\"comics\",1\n",
      "\"comics content\",1\n",
      "\"content\",1\n",
      "\"even\",1\n",
      "\"even appeared\",1\n",
      "\"even appeared recently\",1\n",
      "\"even appeared recently _\",1\n",
      "\"even appeared recently _ graphic\",1\n",
      "\"form\",1\n",
      "\"graphic\",2\n",
      "\"graphic novel\",2\n",
      "\"graphic novel _\",1\n",
      "\"graphic novel _ _\",1\n",
      "\"graphic novel _ _ book\",1\n",
      "\"graphic novel form\",1\n",
      "\"made\",1\n",
      "\"made up\",1\n",
      "\"made up _\",1\n",
      "\"made up _ comics\",1\n",
      "\"made up _ comics content\",1\n",
      "\"novel\",2\n",
      "\"novel _\",1\n",
      "\"novel _ _\",1\n",
      "\"novel _ _ book\",1\n",
      "\"novel _ _ book made\",1\n",
      "\"novel form\",1\n",
      "\"recently\",1\n",
      "\"recently _\",1\n",
      "\"recently _ graphic\",1\n",
      "\"recently _ graphic novel\",1\n",
      "\"recently _ graphic novel form\",1\n",
      "\"up\",1\n",
      "\"up _\",1\n",
      "\"up _ comics\",1\n",
      "\"up _ comics content\",1\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "null"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "//\n",
    "// Exchange data between Java and Python kernels\n",
    "//\n",
    "import java.util.List;\n",
    "\n",
    "String idxUuid = (String)NamespaceClient.getBeakerX().get(\"my_index_uuid\");\n",
    "Integer nbrShards = (Integer) NamespaceClient.getBeakerX().get(\"my_index_shards\");\n",
    "System.out.println(idxUuid);\n",
    "System.out.println(nbrShards);\n",
    "\n",
    "System.out.println();\n",
    "\n",
    "LuceneExplorer luceneExplorer = new LuceneExplorer(idxUuid, nbrShards);\n",
    "List<String> luceneIndices = luceneExplorer.getLucenePaths();\n",
    "System.out.println(luceneIndices);\n",
    "\n",
    "System.out.println();\n",
    "luceneIndices.stream().forEach(idx -> {\n",
    "    try {\n",
    "        luceneExplorer.exploreLuceneIndex(idx);\n",
    "    } catch (Exception e) {\n",
    "        e.printStackTrace();\n",
    "    }\n",
    "});"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('YtgaGpRpTCyDhEtcY6SonQ', 1)"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%python\n",
    "# Retrieve stored data from python kernel\n",
    "from beakerx.object import beakerx\n",
    "\n",
    "beakerx.my_index_uuid, beakerx.my_index_shards"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Java",
   "language": "java",
   "name": "java"
  },
  "language_info": {
   "codemirror_mode": "text/x-java",
   "file_extension": ".java",
   "mimetype": "",
   "name": "Java",
   "nbconverter_exporter": "",
   "version": "1.8.0_202"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": false,
   "sideBar": false,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": false,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
